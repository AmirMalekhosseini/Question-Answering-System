{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Setup: Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOvgOAfIuy3q",
        "outputId": "ae82be14-3e4b-4ddb-fcf8-9b8e4a50a8b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Setup: Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JLzPrTa_ytgj"
      },
      "outputs": [],
      "source": [
        "!pip install sentence-transformers transformers==4.30.0 torch huggingface_hub==0.16.4 torchvision --upgrade -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "id": "0W-1DR3Xl2OL",
        "outputId": "514ca17a-96ba-4cfe-ddd8-bc3140ca57e0"
      },
      "outputs": [],
      "source": [
        "!pip install protobuf==3.20.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip uninstall tensorflow -y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Setup: Authenticate with Hugging Face Hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162,
          "referenced_widgets": [
            "67175e8f94dd412da2eccdb32e07d518",
            "69220a533acd4181aa4b90aa02c34bf9",
            "e1d658e935164365a8e32764a39668e4",
            "dada71a694594b7caf1dbb5b9e63988c",
            "4223625041b9445185bda1c5599656bc",
            "ff7d65f2d0ed4b2eb8919e5c09768534",
            "c051937d03f649e388ca8acdd08b4499",
            "43cf3e4a3d164585bece4616cdbb9e92",
            "dd46f8e1020c43b9901b00cf0fc5ac84",
            "a6ff55a32ee54884abcb304da43b4bb5",
            "357b05820b1c4d84b4f6b8526d8a07a3",
            "54c5f120d70445a19179aca00dc0d98f",
            "2caa0e37bc05467da328dd71bc3243ad",
            "e95dbd6638c74176b79888f475d4c0a0",
            "54f75fb155774874a176f883b411674c",
            "cc6a3d68ddb74ff4b86968eea6c1995c",
            "aec69eb0da5b442bae79216d730995a8",
            "8ec849b615f242b6b8a753b6c8c45e69",
            "0ba10ef8d39044d8a0c0fa5b725f43fb",
            "5868926f0159428c9f3ee11117159fc6",
            "765bb821b5ab4c2fa81d498ecc6d6278",
            "3ac7e3d20f964440870c3c9daa00585c",
            "38ec3eed422f4f0196c21fb9e7c340dd",
            "c62b7db177e849da8a850eabfd51d990",
            "1c4d6c2703ac4d6086dd61e5fdfb9874",
            "a87c3567e08c43be9e1cf3292b50ae31",
            "59ff31248fb54170b1a12bee09cd002c",
            "e932881b352a49cda4dd00ff94d820f4",
            "c79422968feb43c8995389b0ad8bc496",
            "dd561fbb16664f8e958987c754137551",
            "f41b7d9afb2446d8afb216b4e1f2a9fd",
            "de1678c7669c4559bc24007afd8e0aac"
          ]
        },
        "id": "vCmr57w_yz3x",
        "outputId": "fb15cd72-796c-4a13-a608-d2843d7e4b15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please log in to Hugging Face Hub to access gated models:\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "67175e8f94dd412da2eccdb32e07d518",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "print(\"Please log in to Hugging Face Hub to access gated models:\")\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Model Training: Fine-Tune the GLOT500 Model\n",
        "*This cell loads the training data, configures the contrastive learning loss, and fine-tunes the `cis-lmu/glot500-base` model, saving the result to Google Drive.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321,
          "referenced_widgets": [
            "1ccecc7e4f204e15b8345882ca576226",
            "d8159efee9474c5ea44b97d1941ce032",
            "58ba93c94b4a4cff8758daafe3ef498b",
            "33b2bdf53001473fa4828e82496876c1",
            "93fb4befae2143ff956ba277f9668b7f",
            "c42d6c98797e4bf29018cfdfb4422b0f",
            "f23f62ed40374f8a8c2a640ff6210593",
            "f820aa443b85472ab2640f1b7a56539f",
            "f4e97727dc8346b1b31e890b3021872d",
            "b992ae6833024fc48d6e1f888fda96cf",
            "17e2cb5ca135464f836e3d66b5d2536a",
            "581665aefe664ea5929d80eb868002c9",
            "cab973973bc844c6b88abf7324ed1877",
            "75375b2176754ff7b6a0da7d5ce7a3d6",
            "9b3af8e75a404b42ae6b51c3e3a6d4ae",
            "c22fbd535c6042d796dfbba6c4b39649",
            "d931e8ca455746f29af23d9d86e28530",
            "9cf5f4424db34e379e617723da04c6b0",
            "fa836fba7d744744a855d1fcd48a9c5e",
            "c1e89893d69a40e29e25e4a274f504cb",
            "7b2e1009ce1642899d34f60ecce4576c",
            "645b674f56e64c1b9f860656ba3d6ce6",
            "debc3d1f5d574159a6d7bf6b5e5d5bc3",
            "fd4e94b3ec9444129b3c2112f66389ac",
            "5de0ceb2a33b481182f211ca10eaa3ab",
            "bddd6d3c276f4588bb1f56075be3205f",
            "a680573739ac4fc5b3893d8e9f4322b7",
            "48e4545cb45549fd90a5b99023fd5a78",
            "507a1bd94d204d9fb224a913368a1162",
            "61df623ddd60420c8f9ec7c226466264",
            "ff8b78f5b8ae43fab966766fc60e829d",
            "6e143d183daa44c0a3ca7012ccb343fc",
            "4c0d195b96b043918ceebf9f4131aba2",
            "342ac90792d643f1a79224677e2fe66e",
            "6e1c1954b1564698b6931eea127bf503",
            "356670f34a024106acc3e068dfac02d6",
            "59d016f703f64bf196e8905919388d38",
            "992ded1dffc24c2db320c0b33ff94498",
            "f8d1525f83384ae89e55122cbe8390db",
            "996246afb911449fb65c279da19e0a6b",
            "6c4f6004580648b799f31ee42043232a",
            "0755eecc190f4c6ba838664827f0f8a6",
            "56970aabc20b4f869a7d177caac63859",
            "5d5f47f06dcf4a6d84e94d452b10cf23"
          ]
        },
        "id": "JXxmjrYDvZDk",
        "outputId": "3b10397e-f728-4341-c61b-d5392208b47f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/cis-lmu_glot500-base. Creating a new one with MEAN pooling.\n",
            "Some weights of the model checkpoint at /root/.cache/torch/sentence_transformers/cis-lmu_glot500-base were not used when initializing XLMRobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias']\n",
            "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of XLMRobertaModel were not initialized from the model checkpoint at /root/.cache/torch/sentence_transformers/cis-lmu_glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1ccecc7e4f204e15b8345882ca576226",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "581665aefe664ea5929d80eb868002c9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Iteration:   0%|          | 0/82 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "debc3d1f5d574159a6d7bf6b5e5d5bc3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Iteration:   0%|          | 0/82 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "342ac90792d643f1a79224677e2fe66e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Iteration:   0%|          | 0/82 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Part 3: Fine-tuning of 'cis-lmu/glot500-base' complete ---\n",
            "The fine-tuned model and checkpoints should be in: /content/drive/My Drive/NLP_HW2/Models/cis-lmu_glot500-base_finetuned_retrieval_v1\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os \n",
        "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
        "from torch.utils.data import DataLoader\n",
        "import logging\n",
        "from huggingface_hub import model_info \n",
        "import torch\n",
        "\n",
        "\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "logger = logging.getLogger(__name__) \n",
        "logger.info(\"Weights & Biases (wandb) logging has been disabled.\")\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "if not logger.hasHandlers(): \n",
        "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "\n",
        "\n",
        "DRIVE_BASE_PATH = '/content/drive/My Drive/NLP_HW2/'\n",
        "DATA_DIR_NAME = 'Data'        \n",
        "PASSAGES_DIR_NAME = 'Passages'  \n",
        "\n",
        "DATA_PATH = os.path.join(DRIVE_BASE_PATH, DATA_DIR_NAME)\n",
        "TRAINING_QUESTIONS_FILE = os.path.join(DATA_PATH, 'Training_Questions.json')\n",
        "PASSAGES_DIR = os.path.join(DATA_PATH, PASSAGES_DIR_NAME)\n",
        "MODEL_OUTPUT_PATH = os.path.join(DRIVE_BASE_PATH, 'Models/cis-lmu_glot500-base_finetuned_retrieval_v1')\n",
        "\n",
        "# Ensure model output directory exists and paths are correct\n",
        "if not os.path.exists(DRIVE_BASE_PATH):\n",
        "    logger.error(f\"CRITICAL: Google Drive base path does not exist: {DRIVE_BASE_PATH}. Did you run Step 2 (drive.mount)?\")\n",
        "    raise FileNotFoundError(f\"Drive path {DRIVE_BASE_PATH} not found.\")\n",
        "\n",
        "os.makedirs(MODEL_OUTPUT_PATH, exist_ok=True)\n",
        "logger.info(f\"Drive base path: {DRIVE_BASE_PATH}\")\n",
        "logger.info(f\"Data path: {DATA_PATH}\")\n",
        "logger.info(f\"Passages directory: {PASSAGES_DIR}\")\n",
        "logger.info(f\"Model will be saved to: {MODEL_OUTPUT_PATH}\")\n",
        "\n",
        "# Load and Prepare Training Data\n",
        "logger.info(f\"Attempting to load training questions from: {TRAINING_QUESTIONS_FILE}\")\n",
        "try:\n",
        "    with open(TRAINING_QUESTIONS_FILE, 'r', encoding='utf-8') as f:\n",
        "        training_questions_raw = json.load(f)\n",
        "    logger.info(f\"Loaded {len(training_questions_raw)} raw training questions.\")\n",
        "except FileNotFoundError:\n",
        "    logger.error(f\"CRITICAL: Training questions file not found at: {TRAINING_QUESTIONS_FILE}\")\n",
        "    logger.error(f\"Please ensure the file exists and the paths (DATA_DIR_NAME='{DATA_DIR_NAME}') are correct.\")\n",
        "    raise\n",
        "except json.JSONDecodeError:\n",
        "    logger.error(f\"CRITICAL: Error decoding JSON from {TRAINING_QUESTIONS_FILE}. Please check the file format.\")\n",
        "    raise\n",
        "except Exception as e:\n",
        "    logger.error(f\"CRITICAL: An unexpected error occurred while loading training questions: {e}\")\n",
        "    raise\n",
        "\n",
        "train_examples = []\n",
        "not_found_passages = 0\n",
        "empty_passages = 0\n",
        "\n",
        "logger.info(f\"Attempting to process passages from directory: {PASSAGES_DIR}\")\n",
        "if not os.path.isdir(PASSAGES_DIR):\n",
        "    logger.error(f\"CRITICAL: Passages directory not found at: {PASSAGES_DIR}. Please check PASSAGES_DIR_NAME='{PASSAGES_DIR_NAME}'.\")\n",
        "    raise FileNotFoundError(f\"Passages directory {PASSAGES_DIR} not found.\")\n",
        "\n",
        "for item_idx, item in enumerate(training_questions_raw):\n",
        "    question = item.get('question')\n",
        "    passage_filename = item.get('passage_reference')\n",
        "\n",
        "    if not question or not passage_filename:\n",
        "        logger.warning(f\"Skipping item {item_idx+1}/{len(training_questions_raw)} due to missing question or passage_reference: {item}\")\n",
        "        continue\n",
        "\n",
        "    passage_file_path = os.path.join(PASSAGES_DIR, passage_filename)\n",
        "\n",
        "    try:\n",
        "        with open(passage_file_path, 'r', encoding='utf-8') as pf:\n",
        "            passage_text = pf.read()\n",
        "        if not passage_text.strip(): # Check if passage is empty or only whitespace\n",
        "            logger.warning(f\"Passage file {passage_file_path} for question '{question}' (item {item_idx+1}) is empty. Skipping.\")\n",
        "            empty_passages += 1\n",
        "            continue\n",
        "        train_examples.append(InputExample(texts=[question, passage_text]))\n",
        "    except FileNotFoundError:\n",
        "        logger.warning(f\"Passage file not found: {passage_file_path} for question: '{question}' (item {item_idx+1})\")\n",
        "        not_found_passages += 1\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error reading passage file {passage_file_path} (item {item_idx+1}): {e}\")\n",
        "        not_found_passages += 1\n",
        "\n",
        "logger.info(f\"Prepared {len(train_examples)} training examples.\")\n",
        "if not_found_passages > 0:\n",
        "    logger.warning(f\"{not_found_passages} passage files were not found or could not be read.\")\n",
        "if empty_passages > 0:\n",
        "    logger.warning(f\"{empty_passages} passage files were found but were empty.\")\n",
        "\n",
        "if not train_examples:\n",
        "    logger.error(\"CRITICAL: No training examples were prepared. Exiting fine-tuning.\")\n",
        "else:\n",
        "    # Define the Model\n",
        "    model_name = 'cis-lmu/glot500-base'\n",
        "\n",
        "    try:\n",
        "        logger.info(f\"Verifying model '{model_name}' on Hugging Face Hub...\")\n",
        "        model_info_obj = model_info(repo_id=model_name)\n",
        "        logger.info(f\"Model '{model_name}' (ID: {model_info_obj.modelId}) found on Hugging Face Hub.\")\n",
        "        if model_info_obj.gated:\n",
        "             logger.warning(f\"Model '{model_name}' is gated. Ensure you have accepted terms on its Hugging Face page and are logged in (via notebook_login).\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"CRITICAL: Could not find or verify model '{model_name}' on Hugging Face Hub: {e}\")\n",
        "        raise\n",
        "\n",
        "    logger.info(f\"Loading base model: {model_name} using SentenceTransformer.\")\n",
        "    try:\n",
        "        model = SentenceTransformer(model_name)\n",
        "        logger.info(f\"Successfully loaded model: {model_name}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"CRITICAL: Error loading SentenceTransformer model '{model_name}': {e}\")\n",
        "        raise\n",
        "\n",
        "    # Define the Loss Function\n",
        "    train_loss = losses.MultipleNegativesRankingLoss(model=model)\n",
        "    logger.info(f\"Using loss function: {type(train_loss).__name__}\")\n",
        "\n",
        "    # Configure Training\n",
        "    batch_size = 8\n",
        "    num_epochs = 3\n",
        "\n",
        "    steps_per_epoch = (len(train_examples) + batch_size - 1) // batch_size\n",
        "    total_training_steps = steps_per_epoch * num_epochs\n",
        "    warmup_steps = int(total_training_steps * 0.1)\n",
        "\n",
        "    logger.info(f\"Batch size: {batch_size}\")\n",
        "    logger.info(f\"Number of epochs: {num_epochs}\")\n",
        "    logger.info(f\"Total training examples: {len(train_examples)}\")\n",
        "    logger.info(f\"Steps per epoch: {steps_per_epoch}\")\n",
        "    logger.info(f\"Total training steps: {total_training_steps}\")\n",
        "    logger.info(f\"Calculated warmup steps: {warmup_steps}\")\n",
        "\n",
        "    train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=batch_size)\n",
        "\n",
        "    # Train the Model\n",
        "    logger.info(\"Starting model training...\")\n",
        "    try:\n",
        "        model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
        "                  epochs=num_epochs,\n",
        "                  warmup_steps=warmup_steps,\n",
        "                  output_path=MODEL_OUTPUT_PATH,\n",
        "                  show_progress_bar=True,\n",
        "                  checkpoint_path=os.path.join(MODEL_OUTPUT_PATH, 'checkpoints'),\n",
        "                  checkpoint_save_steps=max(1, steps_per_epoch // 2) if steps_per_epoch > 0 else 100,\n",
        "                  checkpoint_save_total_limit=3,\n",
        "                  save_best_model=False\n",
        "                 )\n",
        "        logger.info(f\"Training complete. Model artifacts saved to: {MODEL_OUTPUT_PATH}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"CRITICAL: An error occurred during model training: {e}\")\n",
        "        raise\n",
        "\n",
        "    print(f\"\\n--- Part 3: Fine-tuning of '{model_name}' complete ---\")\n",
        "    print(f\"The fine-tuned model and checkpoints should be in: {MODEL_OUTPUT_PATH}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. Evaluation: Generate Retrieval Results for All Models\n",
        "*This script evaluates the three models (TF-IDF, Zero-shot, and Fine-tuned) against the 50 evaluation questions and saves the top 3 results for each in `Retrieval_Results.json`.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420,
          "referenced_widgets": [
            "52ceea05a080480a956773cfdb313911",
            "e328ca4f9ee1419f90a47427fd2664e7",
            "2ceee7108de04a0c9a069f8e644f033f",
            "cba7c74e51ca48e3bd6e506997849680",
            "363895b68fa2416db2c6f0b3b7fdccef",
            "1724aafe1fa947bc87d957f443451302",
            "d08b44bf00574aa288752897373b8169",
            "7df968cd402a4858983c418925a03b24",
            "43f641b309a745aa83a82b5a4953579a",
            "e2b45c98be5543c19b416c09c61d18df",
            "15cc55f0cdd54decb669b5268fcf1c0e",
            "7797d93975fe43328753f316d8fcf9a6",
            "6394e17a294c4780ad9269dad35c0f25",
            "eda8f27138e3427e935c90658af941f1",
            "6f121734b6344f3d93db7a4d6bd7cd7d",
            "986ae3111bc44cb09aeedbea360121cb",
            "3d4e137d1e0d429dae53c36babb86e69",
            "a11986ebc1314d5a8a473947ebbd9837",
            "ec11a07ac0b1478685edb85c884bce95",
            "fff3c3cf15134e14a4265c2987355a15",
            "e254002ed0e24c418261a444139f0460",
            "84149a76324a4241b89a0b266c21eadd",
            "a4aeaeee481240bea6770f89939b1249",
            "d10690256aba4e7681afca55f8a5bc03",
            "a805b5742bc44159bd1f7eb504eed8ef",
            "41d3d758d4624e70ad80b6a865b629cf",
            "92f90ab6df484a5ba2668c58cf25e4e3",
            "ce8645ce7159468484d318997cc77579",
            "c1e0144f4ba94481a60aa2de43271c50",
            "c1b0fa8dd69e428a922cac0161a2564c",
            "a738c364d76548cbae1616c90cf09a1e",
            "d2510f93e026450796cc5ba3f71ac5dd",
            "146b1fff34384d4f8c00c96302638015",
            "c32a23e758af404d9856b06a565c7b9a",
            "7de5a61ee61741f28b6c8a9a632a18a1",
            "73f2565076b4438ea7cf73d78461a6d3",
            "f40b9cc2a2664e58b66a4856a1aaa265",
            "a47c2819c99647dd9a1f2510affbed8d",
            "50cbea0d3c6e4635bfa24935ea67745d",
            "472ba79b77074111b50d7b7d50bfc40b",
            "b75f2f09898c473786e00646b217a58e",
            "9e24081fe315403a84dc4db3238d15b2",
            "a44f0e0a14094f2bad4b3897d99339c8",
            "d741d539b33a4dc9b07154932689ae2d",
            "4b53273227db4685a7989ce39fd19b20",
            "3adf0c0fd4e94ecdbe227d1a7361091a",
            "906df1b30cb74acc96727f320377315d",
            "162d105347cb44cd94a17fe4d2f5d40b",
            "03f374bbe6ec42fc9851ac873d21c3db",
            "59e8139e708b4eff867de4a96dca5541",
            "42edc2eea5864445926f9c75753e1058",
            "409204098c48436c84778e25a2f4d3e4",
            "4e2515bda23143d0a634d43f6e6eb552",
            "bd3c3dd1141f4c4eb495d5d714acd358",
            "5276cb5c4f3d4d19bfc32e343fd336aa",
            "b69e3bb184634999a90c4f07478da4ca",
            "3121ab8225c2491fbf2298ce718e1889",
            "cb07392e256a493ebadf71ac92ef8dec",
            "6b91459d4b004727aa8992b60f540079",
            "307ad165d8684be49f9eb39f64fd2335",
            "722d998eee754435aa4e23f54e3111c4",
            "c72afff934ab4b8abe397d5425644874",
            "9e4d0f8cf5a44f3aadda56d10a5d742f",
            "025e4d9449ea4a0a9a5b1bff28f82097",
            "b77caefdb63246128043b05865149ca8",
            "95ebde9f65974786870514fff6652783"
          ]
        },
        "id": "pyrX3B2j8QTN",
        "outputId": "e156c4b6-81ab-4c8d-f8b7-2c5620361b24"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/cis-lmu_glot500-base. Creating a new one with MEAN pooling.\n",
            "Some weights of the model checkpoint at /root/.cache/torch/sentence_transformers/cis-lmu_glot500-base were not used when initializing XLMRobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of XLMRobertaModel were not initialized from the model checkpoint at /root/.cache/torch/sentence_transformers/cis-lmu_glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "52ceea05a080480a956773cfdb313911",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7797d93975fe43328753f316d8fcf9a6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a4aeaeee481240bea6770f89939b1249",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading model.safetensors:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c32a23e758af404d9856b06a565c7b9a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading tokenizer_config.json:   0%|          | 0.00/79.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4b53273227db4685a7989ce39fd19b20",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)tencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b69e3bb184634999a90c4f07478da4ca",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py:1081: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Part 4: Retrieval with all three models complete ---\n",
            "Results saved to /content/drive/MyDrive/NLP_HW2/Retrieval_Results.json\n"
          ]
        }
      ],
      "source": [
        "# Part 4: Retrieval Evaluation\n",
        "import json\n",
        "import os\n",
        "import glob\n",
        "import re\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from transformers import pipeline\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Paths\n",
        "DRIVE_BASE_PATH = '/content/drive/MyDrive/NLP_HW2/'\n",
        "DATA_DIR_NAME = 'Data'\n",
        "PASSAGES_DIR_NAME = 'Passages'\n",
        "MODELS_DIR_NAME = 'Models'\n",
        "\n",
        "DATA_PATH = os.path.join(DRIVE_BASE_PATH, DATA_DIR_NAME)\n",
        "EVALUATION_QUESTIONS_FILE = os.path.join(DATA_PATH, 'Evaluation_Questions.json')\n",
        "PASSAGES_DIR = os.path.join(DATA_PATH, PASSAGES_DIR_NAME)\n",
        "FINETUNED_MODEL_PATH = os.path.join(DRIVE_BASE_PATH, MODELS_DIR_NAME, 'cis-lmu_glot500-base_finetuned_retrieval_v1')\n",
        "PART4_OUTPUT_FILE = os.path.join(DRIVE_BASE_PATH, 'Retrieval_Results.json')\n",
        "\n",
        "logger.info(f\"Evaluation questions file: {EVALUATION_QUESTIONS_FILE}\")\n",
        "logger.info(f\"Passages directory: {PASSAGES_DIR}\")\n",
        "logger.info(f\"Fine-tuned model path: {FINETUNED_MODEL_PATH}\")\n",
        "\n",
        "# Helper Functions\n",
        "def extract_relevant_snippet(passage, question, max_length=500):\n",
        "    \"\"\"Extract the most relevant part of passage related to the question\"\"\"\n",
        "    # Persian-aware sentence splitting\n",
        "    sentences = re.split(r'(?<=[.!?؟])\\s+', passage)\n",
        "    sentences = [s.strip() for s in sentences if s.strip()]\n",
        "\n",
        "    if not sentences:\n",
        "        return passage[:max_length] + \"...\" if len(passage) > max_length else passage\n",
        "\n",
        "    try:\n",
        "        vectorizer = TfidfVectorizer()\n",
        "        tfidf = vectorizer.fit_transform([question] + sentences)\n",
        "        question_vec = tfidf[0]\n",
        "        sim_scores = [cosine_similarity(question_vec, tfidf[i+1])[0][0] for i in range(len(sentences))]\n",
        "        most_relevant_idx = np.argmax(sim_scores)\n",
        "\n",
        "        start = max(0, most_relevant_idx - 1)\n",
        "        end = min(len(sentences), most_relevant_idx + 2)\n",
        "        snippet = \" \".join(sentences[start:end])\n",
        "\n",
        "        if len(snippet) > max_length:\n",
        "            snippet = snippet[:max_length].rsplit(' ', 1)[0] + \"...\"\n",
        "        return snippet\n",
        "    except:\n",
        "        return passage[:max_length] + \"...\" if len(passage) > max_length else passage\n",
        "\n",
        "def generate_answer(question, context):\n",
        "    \"\"\"Generate answer using QA model\"\"\"\n",
        "    try:\n",
        "        qa_model = pipeline(\"question-answering\",\n",
        "                          model=\"deepset/xlm-roberta-base-squad2\",\n",
        "                          device=0 if torch.cuda.is_available() else -1)\n",
        "        result = qa_model(question=question, context=context, max_answer_len=100)\n",
        "        return result['answer']\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"Answer generation failed: {str(e)}\")\n",
        "        return \"پاسخ یافت نشد\"\n",
        "\n",
        "# Load Evaluation Questions\n",
        "logger.info(\"Loading evaluation questions...\")\n",
        "try:\n",
        "    with open(EVALUATION_QUESTIONS_FILE, 'r', encoding='utf-8') as f:\n",
        "        evaluation_questions = json.load(f)\n",
        "    logger.info(f\"Loaded {len(evaluation_questions)} evaluation questions.\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error loading evaluation questions: {e}\")\n",
        "    raise\n",
        "\n",
        "# Load Corpus Passages\n",
        "logger.info(f\"Loading corpus passages from: {PASSAGES_DIR}\")\n",
        "corpus_passages = []\n",
        "passage_files = glob.glob(os.path.join(PASSAGES_DIR, \"*.txt\"))\n",
        "\n",
        "if not passage_files:\n",
        "    logger.error(f\"No .txt files found in passages directory: {PASSAGES_DIR}\")\n",
        "    raise FileNotFoundError(f\"No passage files found in {PASSAGES_DIR}\")\n",
        "\n",
        "for passage_file in passage_files:\n",
        "    try:\n",
        "        with open(passage_file, 'r', encoding='utf-8') as pf:\n",
        "            passage_text = pf.read().strip()\n",
        "        if passage_text:\n",
        "            corpus_passages.append(passage_text)\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error reading passage file {passage_file}: {e}\")\n",
        "\n",
        "logger.info(f\"Loaded {len(corpus_passages)} non-empty passages from corpus.\")\n",
        "\n",
        "# TF-IDF Retrieval Setup\n",
        "logger.info(\"Setting up TF-IDF vectorizer...\")\n",
        "vectorizer = TfidfVectorizer()\n",
        "corpus_tfidf_matrix = vectorizer.fit_transform(corpus_passages)\n",
        "logger.info(f\"TF-IDF matrix shape: {corpus_tfidf_matrix.shape}\")\n",
        "\n",
        "# Load Models\n",
        "logger.info(\"Loading models...\")\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Base (zero-shot) model\n",
        "BASE_MODEL_NAME = 'cis-lmu/glot500-base'\n",
        "try:\n",
        "    base_model = SentenceTransformer(BASE_MODEL_NAME, device=device)\n",
        "    logger.info(f\"Encoding passages with base model ({device})...\")\n",
        "    corpus_embeddings_base_model = base_model.encode(corpus_passages,\n",
        "                                                    convert_to_tensor=True,\n",
        "                                                    show_progress_bar=True,\n",
        "                                                    batch_size=32)\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error loading base model: {e}\")\n",
        "    raise\n",
        "\n",
        "# Fine-tuned model\n",
        "try:\n",
        "    finetuned_model = SentenceTransformer(FINETUNED_MODEL_PATH, device=device)\n",
        "    logger.info(f\"Encoding passages with fine-tuned model ({device})...\")\n",
        "    corpus_embeddings_finetuned_model = finetuned_model.encode(corpus_passages,\n",
        "                                                             convert_to_tensor=True,\n",
        "                                                             show_progress_bar=True,\n",
        "                                                             batch_size=32)\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error loading fine-tuned model: {e}\")\n",
        "    raise\n",
        "\n",
        "# Initialize QA model\n",
        "logger.info(\"Loading QA model for answer generation...\")\n",
        "qa_pipeline = pipeline(\"question-answering\",\n",
        "                      model=\"deepset/xlm-roberta-base-squad2\",\n",
        "                      device=0 if torch.cuda.is_available() else -1)\n",
        "\n",
        "# Perform Retrieval and Generate Results \n",
        "final_results = []\n",
        "\n",
        "logger.info(\"Starting retrieval and answer generation...\")\n",
        "for question_data in evaluation_questions:\n",
        "    question_id = question_data[\"evaluation_question_id\"]\n",
        "    question_text = question_data[\"question\"]\n",
        "    province = question_data.get(\"expected_province_for_answer\", \"\")\n",
        "\n",
        "    if not question_text:\n",
        "        continue\n",
        "\n",
        "    result = {\n",
        "        \"evaluation_question_id\": question_id,\n",
        "        \"question_text\": question_text,\n",
        "        \"expected_province\": province,\n",
        "        \"tfidf_top3\": [],\n",
        "        \"zeroshot_glot500_top3\": [],\n",
        "        \"finetuned_glot500_top3\": [],\n",
        "        \"generated_answer\": \"\"\n",
        "    }\n",
        "\n",
        "    # TF-IDF Retrieval\n",
        "    question_tfidf = vectorizer.transform([question_text])\n",
        "    similarities = cosine_similarity(question_tfidf, corpus_tfidf_matrix)[0]\n",
        "    top_indices = np.argsort(similarities)[-3:][::-1]\n",
        "\n",
        "    for rank, idx in enumerate(top_indices, 1):\n",
        "        result[\"tfidf_top3\"].append({\n",
        "            \"passage_text_snippet\": extract_relevant_snippet(corpus_passages[idx], question_text),\n",
        "            \"score\": float(similarities[idx]),\n",
        "            \"rank\": rank\n",
        "        })\n",
        "\n",
        "    # Zero-shot Retrieval\n",
        "    question_embedding = base_model.encode(question_text, convert_to_tensor=True)\n",
        "    similarities = util.cos_sim(question_embedding, corpus_embeddings_base_model)[0]\n",
        "    top_indices = torch.topk(similarities, k=3).indices\n",
        "\n",
        "    for rank, idx in enumerate(top_indices, 1):\n",
        "        result[\"zeroshot_glot500_top3\"].append({\n",
        "            \"passage_text_snippet\": extract_relevant_snippet(corpus_passages[idx], question_text),\n",
        "            \"score\": similarities[idx].item(),\n",
        "            \"rank\": rank\n",
        "        })\n",
        "\n",
        "    # Fine-tuned Retrieval\n",
        "    question_embedding = finetuned_model.encode(question_text, convert_to_tensor=True)\n",
        "    similarities = util.cos_sim(question_embedding, corpus_embeddings_finetuned_model)[0]\n",
        "    top_indices = torch.topk(similarities, k=3).indices\n",
        "\n",
        "    for rank, idx in enumerate(top_indices, 1):\n",
        "        result[\"finetuned_glot500_top3\"].append({\n",
        "            \"passage_text_snippet\": extract_relevant_snippet(corpus_passages[idx], question_text),\n",
        "            \"score\": similarities[idx].item(),\n",
        "            \"rank\": rank\n",
        "        })\n",
        "\n",
        "    # Generate answer using top passage from fine-tuned model\n",
        "    if result[\"finetuned_glot500_top3\"]:\n",
        "        top_passage_idx = top_indices[0].item()\n",
        "        context = corpus_passages[top_passage_idx]\n",
        "        try:\n",
        "            qa_result = qa_pipeline(question=question_text, context=context, max_answer_len=100)\n",
        "            result[\"generated_answer\"] = qa_result['answer']\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Answer generation failed for Q{question_id}: {str(e)}\")\n",
        "            result[\"generated_answer\"] = \"پاسخ یافت نشد\"\n",
        "\n",
        "    final_results.append(result)\n",
        "    logger.info(f\"Processed question {question_id}/{len(evaluation_questions)}\")\n",
        "\n",
        "# Save Results \n",
        "logger.info(f\"Saving final results to {PART4_OUTPUT_FILE}\")\n",
        "try:\n",
        "    with open(PART4_OUTPUT_FILE, 'w', encoding='utf-8') as f:\n",
        "        json.dump(final_results, f, indent=2, ensure_ascii=False)\n",
        "    logger.info(\"Results saved successfully.\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error saving results: {e}\")\n",
        "    raise\n",
        "\n",
        "print(f\"\\n--- Part 4: Retrieval with all three models complete ---\")\n",
        "print(f\"Results saved to {PART4_OUTPUT_FILE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKEXEtOAsiuo"
      },
      "source": [
        "### 6. Bonus Task: Duplicate Passage Detection\n",
        "*This script uses the fine-tuned model to find and report passages in the corpus that are highly similar to each other.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205,
          "referenced_widgets": [
            "a60fa50f27f143c9b83cd43249eee586",
            "d9798808e8f847f280cddcada2873dcd",
            "ba1d971345b345e68c2b4a01b3b2e259",
            "59dba488ed6c403a83e272a2e7c682c4",
            "a8f8dc25fb8b46f59bf82fbdd7dfdb17",
            "196b45f718ca40fdb9b60a54263a17a7",
            "79e80bce433c460fa38e0a33931fe56e",
            "ffdc5a6a8f11477dbea4655e9014cad7",
            "8b6e888bbdea4d3f87b38673ab642812",
            "1e1e10b9db0848c8bebd80dd92c31497",
            "d61f0de96a5a4ce38ffa17eeced7e301"
          ]
        },
        "id": "4WR-cBL-sQXA",
        "outputId": "6bc3fadb-f498-4037-ef55-966f6121db63"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a60fa50f27f143c9b83cd43249eee586",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\\n==================================================\n",
            "              DUPLICATE DETECTION RESULTS\n",
            "==================================================\\n\n",
            "Score: 0.9287\n",
            "File 1: Ardabil_Summary.txt\n",
            "File 2: East_Azerbaijan_Summary.txt\n",
            "Text 1 Snippet: 'استان اردبیل، با مرکزیت شهر اردبیل، یکی از استان‌های شمال غربی ایران است که به دلیل آب و هوای سرد و ...'\n",
            "Text 2 Snippet: 'استان آذربایجان شرقی، با مرکزیت شهر تبریز، یکی از استان‌های بزرگ و مهم در شمال غربی ایران است. این ا...'\n",
            "--------------------------------------------------\\n\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "import os\n",
        "import glob\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch\n",
        "import logging\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "try:\n",
        "    \n",
        "    FINETUNED_MODEL_PATH\n",
        "    PASSAGES_DIR\n",
        "except NameError:\n",
        "    logger.warning(\"Paths not defined, defining them now. Make sure they are correct.\")\n",
        "    DRIVE_BASE_PATH = '/content/drive/My Drive/NLP_HW2/'\n",
        "    DATA_DIR_NAME = 'Data'\n",
        "    PASSAGES_DIR_NAME = 'Passages'\n",
        "    MODELS_DIR_NAME = 'Models'\n",
        "    DATA_PATH = os.path.join(DRIVE_BASE_PATH, DATA_DIR_NAME)\n",
        "    PASSAGES_DIR = os.path.join(DATA_PATH, PASSAGES_DIR_NAME)\n",
        "    FINETUNED_MODEL_PATH = os.path.join(DRIVE_BASE_PATH, MODELS_DIR_NAME, 'cis-lmu_glot500-base_finetuned_retrieval_v1')\n",
        "\n",
        "logger.info(f\"Using fine-tuned model from: {FINETUNED_MODEL_PATH}\")\n",
        "logger.info(f\"Loading passages from: {PASSAGES_DIR}\")\n",
        "\n",
        "# Load the Fine-Tuned Model\n",
        "try:\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    model = SentenceTransformer(FINETUNED_MODEL_PATH, device=device)\n",
        "    logger.info(f\"Successfully loaded fine-tuned model on device: {device}\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Failed to load the fine-tuned model. Ensure training was successful and the path is correct.\")\n",
        "    raise e\n",
        "\n",
        "# Load All Passages \n",
        "passage_files = glob.glob(os.path.join(PASSAGES_DIR, \"*.txt\"))\n",
        "corpus_passages = {} \n",
        "\n",
        "for passage_file in passage_files:\n",
        "    try:\n",
        "        with open(passage_file, 'r', encoding='utf-8') as pf:\n",
        "            passage_text = pf.read().strip()\n",
        "        if passage_text:\n",
        "            corpus_passages[os.path.basename(passage_file)] = passage_text\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error reading passage file {passage_file}: {e}\")\n",
        "\n",
        "logger.info(f\"Loaded {len(corpus_passages)} non-empty passages.\")\n",
        "\n",
        "corpus_filenames = list(corpus_passages.keys())\n",
        "corpus_texts = list(corpus_passages.values())\n",
        "\n",
        "# Encode All Passages \n",
        "logger.info(\"Encoding all passages with the fine-tuned model. This may take a moment...\")\n",
        "corpus_embeddings = model.encode(corpus_texts,\n",
        "                                 convert_to_tensor=True,\n",
        "                                 show_progress_bar=True,\n",
        "                                 batch_size=32)\n",
        "\n",
        "logger.info(f\"Encoding complete. Shape of embeddings tensor: {corpus_embeddings.shape}\")\n",
        "\n",
        "# Find and Display Duplicate Pairs\n",
        "score_threshold = 0.90\n",
        "\n",
        "logger.info(f\"Finding duplicate pairs with a similarity score > {score_threshold}...\")\n",
        "duplicate_pairs = util.paraphrase_mining_embeddings(corpus_embeddings,\n",
        "                                                    top_k=5,\n",
        "                                                    query_chunk_size=5000, \n",
        "                                                    corpus_chunk_size=100000)\n",
        "\n",
        "logger.info(f\"Found {len(duplicate_pairs)} potential duplicate pairs above the threshold.\")\n",
        "\n",
        "print(\"\\\\n\" + \"=\"*50)\n",
        "print(\"              DUPLICATE DETECTION RESULTS\")\n",
        "print(\"=\"*50 + \"\\\\n\")\n",
        "\n",
        "if not duplicate_pairs:\n",
        "    print(f\" No duplicate pairs found with a similarity score above {score_threshold}.\")\n",
        "else:\n",
        "    # Sort pairs by score for cleaner output\n",
        "    duplicate_pairs.sort(key=lambda x: x[0], reverse=True)\n",
        "\n",
        "    for score, i, j in duplicate_pairs:\n",
        "        # Check if score is above our defined threshold\n",
        "        if score > score_threshold:\n",
        "            file1 = corpus_filenames[i]\n",
        "            file2 = corpus_filenames[j]\n",
        "\n",
        "            if file1 < file2:\n",
        "                print(f\"Score: {score:.4f}\")\n",
        "                print(f\"File 1: {file1}\")\n",
        "                print(f\"File 2: {file2}\")\n",
        "                print(f\"Text 1 Snippet: '{corpus_texts[i][:100]}...'\")\n",
        "                print(f\"Text 2 Snippet: '{corpus_texts[j][:100]}...'\")\n",
        "                print(\"-\"*50 + \"\\\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8. Annotation Prep: Format Results for Label Studio\n",
        "*This script takes the model outputs, shuffles them to prevent bias, and formats them into a JSON file for import into Label Studio.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMvxK27xt2Af",
        "outputId": "dbf64ad9-7062-4864-a37f-7a8804a7f048"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import random\n",
        "\n",
        "input_file = \"/content/drive/MyDrive/NLP_HW2/Retrieval_Results.json\"\n",
        "output_file = \"/content/drive/MyDrive/NLP_HW2//label_studio_ready.json\"\n",
        "\n",
        "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "output_data = []\n",
        "\n",
        "for item in data:\n",
        "    question = item.get(\"question_text\")\n",
        "    all_passages = (\n",
        "        item.get(\"tfidf_top3\", []) +\n",
        "        item.get(\"zeroshot_glot500_top3\", []) +\n",
        "        item.get(\"finetuned_glot500_top3\", [])\n",
        "    )\n",
        "\n",
        "    # Deduplicate\n",
        "    seen = set()\n",
        "    unique_snippets = []\n",
        "    for p in all_passages:\n",
        "        text = p[\"passage_text_snippet\"]\n",
        "        if text not in seen:\n",
        "            seen.add(text)\n",
        "            unique_snippets.append(text)\n",
        "\n",
        "    while len(unique_snippets) < 9:\n",
        "        unique_snippets.append(\"پاسخ تکراری\")\n",
        "\n",
        "    selected = unique_snippets[:9]\n",
        "    random.shuffle(selected)\n",
        "\n",
        "    task = {\"question\": question}\n",
        "    for i, passage in enumerate(selected):\n",
        "        task[f\"ans{i+1}\"] = passage\n",
        "\n",
        "    output_data.append(task)\n",
        "\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(output_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"Saved {len(output_data)} tasks to {output_file}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "version": "3.8.5"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
